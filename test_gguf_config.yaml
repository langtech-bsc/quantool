model_id: "amd/AMD-Llama-135m"
tokenizer_name: null
cache_dir: null
use_auth_token: false
revision: null

method: "gguf"
quant_level: ["Q4_K_M", "Q3_K_S"]
# quantization_config is dictionary with quantization parameters
quantization_config:
  llama_cpp_path: null

output_path: "./output/test_gguf"
push_to_hub: true
repo_id: langtech-mlops/test_gguf
private: true

seed: 42
verbose: true

report_to: null
log_level: "INFO"
save_logs: true
log_dir: "./logs"

enable_evaluation: false
eval_dataset: null
metrics: ["perplexity"]