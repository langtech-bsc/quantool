model_id: "amd/AMD-Llama-135m"
tokenizer_name: null
cache_dir: null
use_auth_token: false
revision: null

calibration_config:
method: "gptq"
quant_level: "W8A8"
quantization_config:
  targets: "Linear"
  tokenizer: "amd/AMD-Llama-135m"
  ignore: ["lm_head"]
  dataset: "HuggingFaceH4/ultrachat_200k"
  num_calibration_samples: 4
  max_seq_length: 512
  shuffle_calibration_samples: true


output_path: "./output/test_gptq"
push_to_hub: false
repo_id: null
private: true

seed: 42
verbose: true

report_to: null
log_level: "INFO"
save_logs: true
log_dir: "./logs"

enable_evaluation: false
eval_dataset: null
metrics: ["perplexity"]
